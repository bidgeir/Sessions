{"cells":[{"cell_type":"code","source":["# Utilities\n","from pyspark.sql.functions import trim,when,isnull,lit,col,from_utc_timestamp,to_utc_timestamp,concat_ws,sha1,length,substring,lit,concat,date_add,expr,year,datediff, explode, explode_outer\n","from pyspark.sql import functions as F \n","from pyspark.sql.types import *\n","import datetime\n","\n","# COMMAND ----------\n","\n","class CommonTransforms:\n","  inputDf=None\n","  inputSchema=None\n","  inputColums=None\n","  \n","#   Constructor\n","  def __init__(self, input):\n","    self.inputDf=input\n","    self.inputSchema=self.inputDf.schema\n","    self.inputColumns=self.inputDf.schema.fieldNames()\n","    \n","#  Remove Leading and Trailing Spaces \n","  def trim(self):\n","    stringCol= (col for col in self.inputSchema if str(col.dataType)==\"StringType\")\n","    for col in stringCol:\n","        self.inputDf = self.inputDf.withColumn(col.name,trim(col.name))\n","    return self.inputDf\n","  \n","#   Replace Null values with Default values based on datatypes\n","  def replaceNull(self,value, subset=None):\n","    isDate=False\n","    isTimestamp =False\n","    \n","    try:\n","      if isinstance(value, str):\n","        date_obj = datetime.datetime.strptime(value, \"%Y-%m-%d\") #YYYY-MM-DD format e.g \"2020-10-01\"\n","        isDate= True\n","    except ValueError:\n","      isDate=False\n","      \n","    try:\n","      if isinstance(value, str):\n","        date_obj = datetime.datetime.strptime(value, \"%Y-%m-%dT%H:%M:%S\") #YYYY-MM-DDThh:mm:ss format e.g \"2020-10-01T19:50:06\"\n","        isTimestamp= True\n","    except ValueError:\n","      isTimestamp=False\n","      \n","    if isDate and subset is not None:\n","      dateCol = (x for x in self.inputSchema if str(x.dataType)==\"DateType\" and x.nullable==True and x.name in subset)\n","      for x in dateCol:\n","        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))\n","    elif isDate and subset is None:\n","      dateCol = (x for x in self.inputSchema if str(x.dataType)==\"DateType\" and x.nullable==True)\n","      for x in dateCol:\n","        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))\n","    elif isTimestamp and subset is not None:\n","      tsCol = (x for x in self.inputSchema if str(x.dataType)==\"TimestampType\" and x.nullable==True and x.name in subset)\n","      for x in tsCol:\n","        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))\n","    elif isTimestamp and subset is None:\n","      tsCol = (x for x in self.inputSchema if str(x.dataType)==\"TimestampType\" and x.nullable==True)\n","      for x in tsCol:\n","        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))        \n","    else:\n","      self.inputDf = self.inputDf.fillna(value,subset)\n","      \n","    return self.inputDf\n","\n","#  Remove duplicates\n","  def deDuplicate(self, subset=None):\n","    self.inputDf = self.inputDf.dropDuplicates(subset)\n","    return self.inputDf\n","  \n","#   Convert UTC timestamp to local\n","  def utc_to_local(self,localTimeZone,subset=None):\n","    if subset is not None:\n","      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\" and x.name in subset)\n","    else:\n","      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\")\n","      \n","    for x in tsCol:\n","      self.inputDf = self.inputDf.withColumn(x.name,from_utc_timestamp(col(x.name),localTimeZone))\n","    return self.inputDf\n","\n","#   Convert timestamp in local timezone to UTC\n","  def local_to_utc(self,localTimeZone,subset=None):\n","    if subset is not None:\n","      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\" and x.name in subset)\n","    else:\n","      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\")\n","      \n","    for x in tsCol:\n","      self.inputDf = self.inputDf.withColumn(x.name,to_utc_timestamp(col(x.name),localTimeZone))\n","    return self.inputDf\n","  \n","#   Change Timezone\n","  def changeTimezone(self,fromTimezone,toTimezone,subset=None):\n","    if subset is not None:\n","      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\" and x.name in subset)\n","    else:\n","      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\")\n","    \n","    for x in tsCol:\n","      self.inputDf = self.inputDf.withColumn(x.name,to_utc_timestamp(col(x.name),fromTimezone))\n","      self.inputDf = self.inputDf.withColumn(x.name,from_utc_timestamp(col(x.name),toTimezone))\n","    return self.inputDf\n","\n","#   Drop System/Non-Business Columns\n","  def dropSysColumns(self,columns):\n","    self.inputDf = self.inputDf.drop(columns)\n","    return self.inputDf \n","  \n","#  Create Checksum Column \n","  def addChecksumCol(self,colName):\n","    self.inputDf = self.inputDf.withColumn(colName,sha1(concat_ws(\"~~\", *self.inputDf.columns)))\n","    return self.inputDf\n","\n","# Convert Julian Date to Calendar Date  \n","  def julian_to_calendar(self,subset):\n","    julCol = (x for x in self.inputSchema if str(x.dataType)==\"IntegerType\" and x.name in subset)\n","    for x in julCol:\n","      self.inputDf = (self.inputDf.withColumn(x.name,col(x.name).cast(\"string\"))\n","                                 .withColumn(x.name+\"_year\",\n","                                             when((length(col(x.name))==5) & (substring(col(x.name),1,2) <=50),concat(lit('20'),substring(col(x.name),1,2)))\n","                                             .when((length(col(x.name))==5) & (substring(col(x.name),1,2) >50),concat(lit('19'),substring(col(x.name),1,2)))\n","                                             .when(length(col(x.name))==7,substring(col(x.name),1,4))\n","                                             .otherwise(lit(0))\n","                                            )\n","                                 .withColumn(x.name+\"_days\",\n","                                             when(length(col(x.name))==5,substring(col(x.name),3,3).cast(\"int\"))\n","                                             .when(length(col(x.name))==7,substring(col(x.name),5,3).cast(\"int\"))\n","                                             .otherwise(lit(0))\n","                                            )\n","                                 .withColumn(x.name+\"_ref_year\",concat(col(x.name+\"_year\"),lit(\"-01\"),lit(\"-01\")).cast(\"date\"))\n","                                 .withColumn(x.name+\"_calendar\",expr(\"date_add(\" + x.name+\"_ref_year\"+\",\"+ x.name+\"_days)-1\"))\n","                                 .drop(x.name, x.name+\"_year\",x.name+\"_days\",x.name+\"_ref_year\")\n","                                 .withColumnRenamed(x.name+\"_calendar\",x.name)\n","                                 \n","                     )\n","    return self.inputDf \n","  \n","# Convert Calendar Date to Julian Date \n","  def calendar_to_julian(self, subset):\n","    calCol = (x for x in self.inputSchema if ((str(x.dataType)==\"DateType\" or str(x.dataType)==\"TimestampType\") and x.name in subset))\n","\n","    for x in calCol:\n","      self.inputDf = (self.inputDf.withColumn(x.name+\"_ref_year\", concat(year(col(x.name)).cast(\"string\"),lit(\"-01\"),lit(\"-01\")))\n","                                  .withColumn(x.name+\"_datediff\", datediff(col(x.name),col(x.name+\"_ref_year\"))+1)\n","                                  .withColumn(x.name+\"_julian\", concat(substring(year(col(x.name)).cast(\"string\"),3,2),col(x.name+\"_datediff\")).cast(\"int\"))\n","                                  .drop(x.name,x.name+\"_ref_year\",x.name+\"_datediff\")\n","                                  .withColumnRenamed(x.name+\"_julian\",x.name)\n","                     )\n","    return self.inputDf\n","\n","# Add a set of literal value columns to dataframe, pass as dictionary parameter  \n","  def addLitCols(self,colDict):\n","    for x in colDict.items():\n","      self.inputDf = self.inputDf.withColumn(x[0],lit(x[1]))\n","    return self.inputDf\n","\n","# Flatten JSON\n","  def flatten(self):\n","   # compute Complex Fields (Lists and Structs) in Schema\n","   df = self.inputDf\n","   complex_fields = dict([(field.name, field.dataType)\n","                             for field in df.schema.fields\n","                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n","   while len(complex_fields)!=0:\n","      col_name=list(complex_fields.keys())[0]\n","      print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\n","    \n","      # if StructType then convert all sub element to columns.\n","      # i.e. flatten structs\n","      if (type(complex_fields[col_name]) == StructType):\n","         expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\n","         df=df.select(\"*\", *expanded).drop(col_name)\n","    \n","      # if ArrayType then add the Array Elements as Rows using the explode function\n","      # i.e. explode Arrays\n","      elif (type(complex_fields[col_name]) == ArrayType):    \n","         df=df.withColumn(col_name,explode_outer(col_name))\n","    \n","      # recompute remaining Complex Fields in Schema       \n","      complex_fields = dict([(field.name, field.dataType)\n","                             for field in df.schema.fields\n","                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n","   return df\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad21e648-5924-4b67-b742-02c8a2357ad7"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"python"},"widgets":{},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"kernel_info":{"name":"synapse_pyspark"},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}